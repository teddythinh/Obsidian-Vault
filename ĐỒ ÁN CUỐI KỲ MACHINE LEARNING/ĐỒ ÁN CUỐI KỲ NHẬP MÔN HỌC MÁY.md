# Giới thiệu
Trend hiện nay -> Các mô hình ngôn ngữ (ngôn ngữ nói chung, không chỉ riêng ngôn ngữ tự nhiên) 

Mô hình Transformers
- BERT, PhoBERT
- BART 
- **GPT1, GPT2** 

Chỉ cần tìm hiểu cách làm, kiến trúc Encoder - Decoder, chỉ cần hiểu một chút toán 
Đọc paper về Transformer, đọc phần Methods 

Đề cập phương pháp học truyền thống, sau đề cập đến phương pháp học bằng mạng neuron (Transformers) 
# Resources
[https://huggingface.co/tasks/text-generation](https://huggingface.co/tasks/text-generation?fbclid=IwAR3mzaj6Y1BC0o_3QeNaSNHxxjBKcBTg32iMEjMjj3m7lfEvBVAaz9JFXYA) 

[https://paperswithcode.com/methods/category/transformers](https://paperswithcode.com/methods/category/transformers?fbclid=IwAR3DbCaaRbmoMN-Ef7GtdkjkDyMrW8wFX0QSrRCYI0YocPn5Qiv2UGX-470) 

[https://paperswithcode.com/method/transformer](https://paperswithcode.com/method/transformer?fbclid=IwAR0CF-YNUO5iayZ5SrkBR38UnUrtO3glvVmZis_TZYWaoeXvW9CFri-eGCw)

- **[GPT: "Improving Language Understanding by Generative Pre-Training"](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)**
- **[GPT-2: "Language Models are Unsupervised Multitask Learners"](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)**
- **[GPT-3: "Language Models are Few-Shot Learners"](https://arxiv.org/pdf/2005.14165.pdf)**
- **[InstructGPT: "Training language models to follow instructions
with human feedback"](https://arxiv.org/pdf/2203.02155.pdf)**
- **[ChatGPT](https://openai.com/blog/chatgpt)**
- **[GPT-4: "GPT-4 Technical Report"](https://arxiv.org/pdf/2303.08774.pdf)**
# Contexts
- [[MÔ HÌNH TRANSFORMERS]]

